{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rlixne56HZiS"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]='4'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5pHivV1HZiW"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import skimage.exposure\n",
        "from numpy.random import default_rng\n",
        "import random\n",
        "import numpy as np\n",
        "import skimage\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from resnet_vit_keras import vit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "  tf.config.experimental.set_memory_growth(gpu, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Liqzo0gtHZie"
      },
      "source": [
        "### Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img_w = 2048\n",
        "img_h = 128\n",
        "channels = 3\n",
        "num_classes = 50\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def noise_function(image):\n",
        "    image = image / 255.0\n",
        "    image = skimage.util.random_noise(image, mode='s&p', amount=1)\n",
        "    image = image * 255\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def damage_function(image): \n",
        "    patch_size = 32\n",
        "    num_patches = img_h // patch_size * img_w // patch_size\n",
        "\n",
        "    for i in range(num_patches):\n",
        "        prob = 0.2\n",
        "        if random.choices([0, 1], [1-prob, prob])[0]:\n",
        "            image[(i//(img_w // patch_size))*patch_size:(i//(img_w // patch_size))*patch_size+patch_size, (i%(img_w // patch_size))*patch_size:(i%(img_w // patch_size))*patch_size+patch_size, :] = np.zeros([patch_size, patch_size, channels])\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def random_damage_function(image):\n",
        "    image = image.astype(np.uint8)\n",
        "    height, width = image.shape[:2]\n",
        "\n",
        "    # define random seed to change the pattern\n",
        "    rng = default_rng()\n",
        "\n",
        "    # create random noise image\n",
        "    noise = rng.integers(0, 255, (height,width), np.uint8, True)\n",
        "\n",
        "    # blur the noise image to control the size\n",
        "    blur = cv2.GaussianBlur(noise, (0,0), sigmaX=15, sigmaY=15, borderType = cv2.BORDER_DEFAULT)\n",
        "\n",
        "    # stretch the blurred image to full dynamic range\n",
        "    stretch = skimage.exposure.rescale_intensity(blur, in_range='image', out_range=(0,255)).astype(np.uint8)\n",
        "\n",
        "    # threshold stretched image to control the size\n",
        "    thresh = cv2.threshold(stretch, 175, 255, cv2.THRESH_BINARY)[1]\n",
        "\n",
        "    # apply morphology open and close to smooth out and make 3 channels\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9,9))\n",
        "    mask = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
        "    mask = cv2.merge([mask,mask,mask])\n",
        "\n",
        "    # add mask to input\n",
        "    result = cv2.add(image, mask)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    # preprocessing_function = random_damage_function\n",
        ")    \n",
        "\n",
        "train_ds = image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                directory='../../datasets/train',\n",
        "                                                shuffle=True,\n",
        "                                                target_size=(img_h, img_w),\n",
        "                                                class_mode = 'categorical',)\n",
        "\n",
        "validation_ds = image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                    directory='../../datasets/val',\n",
        "                                                    shuffle=False,\n",
        "                                                    target_size=(img_h, img_w),\n",
        "                                                    class_mode = 'categorical')\n",
        "\n",
        "test_ds = image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                directory='../../datasets/test',\n",
        "                                                shuffle=False,\n",
        "                                                target_size=(img_h, img_w),\n",
        "                                                class_mode = 'categorical')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "patch_size = 16\n",
        "num_patches = img_h // patch_size * img_w // patch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Patches(tf.keras.layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image = next(train_ds)[0][0]/255.0\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "fig.patch.set_facecolor('black')\n",
        "plt.imshow(image)\n",
        "plt.axis(\"off\")\n",
        "plt.savefig('./sample_data.png')\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "resized_image = tf.image.resize(tf.convert_to_tensor([image]), size=(img_h, img_w))\n",
        "patches = Patches(patch_size)(resized_image)\n",
        "print(f\"Image size: {img_h} X {img_w}\")\n",
        "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
        "print(f\"Patches per image: {patches.shape[1]}\")\n",
        "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
        "\n",
        "n = img_w // patch_size\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "fig.patch.set_facecolor('black')\n",
        "plt.subplots_adjust(left=0.1,\n",
        "                bottom=0.1,\n",
        "                right=0.9,\n",
        "                top=0.9,\n",
        "                wspace=0.3,\n",
        "                hspace=0.3)\n",
        "for i, patch in enumerate(patches[0]):\n",
        "    ax = plt.subplot(n, n, i + 1)\n",
        "    patch_img = tf.reshape(patch, (patch_size, patch_size, channels))\n",
        "    plt.imshow(patch_img)\n",
        "    plt.axis(\"off\")\n",
        "plt.savefig('./patches.png')\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_shape = (img_h, img_w, channels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZraYJOEHZjN"
      },
      "outputs": [],
      "source": [
        "def create_resnet_vit_classifier():\n",
        "    model = vit.build_model(\n",
        "                image_size = (img_h, img_w),\n",
        "                patch_size = patch_size,\n",
        "                num_layers = 12,\n",
        "                hidden_size = 768,\n",
        "                num_heads = 12,\n",
        "                name = 'resnet_vit',\n",
        "                mlp_dim = 3072,\n",
        "                classes = num_classes,\n",
        "                dropout = 0.1,\n",
        "                activation = 'softmax'\n",
        "            )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resnet_vit_classifier = create_resnet_vit_classifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resnet_vit_classifier.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(\n",
        "    resnet_vit_classifier,\n",
        "    to_file='./model.png',\n",
        "    show_shapes=True,\n",
        "    show_dtype=True,\n",
        "    show_layer_names=True,\n",
        "    rankdir='TB',\n",
        "    expand_nested=True,\n",
        "    dpi=96,\n",
        "    layer_range=None,\n",
        "    show_layer_activations=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compile, train, and evaluate the mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate = 0.00001\n",
        "num_epochs = 100\n",
        "checkpoint_filepath = f\"../../weights/ResNet_ViT/{patch_size}/checkpoint\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "resnet_vit_classifier.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
        "    metrics=[\n",
        "        tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_experiment(model):\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.ModelCheckpoint(checkpoint_filepath, monitor=\"val_accuracy\", save_best_only=True, save_weights_only=True),\n",
        "    ]\n",
        "\n",
        "    history = model.fit(\n",
        "        train_ds,\n",
        "        epochs=num_epochs,\n",
        "        validation_data=validation_ds,\n",
        "        callbacks=callbacks,\n",
        "    )\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = run_experiment(resnet_vit_classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('ResNet_ViT loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.savefig('./loss.png')\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('ResNet_ViT accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='lower right')\n",
        "plt.savefig('./accuracy.png')\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def model_evaluate(model, test_ds):\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "    _, accuracy = model.evaluate(test_ds)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    with open('results.txt', 'w') as f:\n",
        "        f.write(f\"Test accuracy: {round(accuracy * 100, 2)}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_evaluate(resnet_vit_classifier, test_ds)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "image_classification_with_vision_transformer",
      "provenance": [],
      "toc_visible": true
    },
    "environment": {
      "name": "tf2-gpu.2-4.m61",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m61"
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 ('norhand')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "823f7b2671db007787d0db0546ea81831f91cc44f11498d06f7d8c9536fb94b9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
